{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intallation and Imports\n",
    "\n",
    "# OpenAI (v0.28), Pandas, and tqdm\n",
    "!pip -q install \"openai==0.28\" pandas tqdm \n",
    "\n",
    "import os, json, gzip, time, difflib\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from typing import Dict, Any, List, Tuple\n",
    "from getpass import getpass\n",
    "import openai\n",
    "import random\n",
    "import copy\n",
    "\n",
    "print(\"OpenAI SDK version:\", openai.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API Key Request\n",
    "\n",
    "openai.api_key = getpass(\"Enter your OpenAI API key (will not echo): \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "\n",
    "FILE_PATH         = \"DATA/abcd_v1.1.json\"   \n",
    "ONTOLOGY_PATH     = \"DATA/ontology.json\" # optional; enrich label sets if present\n",
    "PRIMARY_MODEL     = \"gpt-4o\" \n",
    "FALLBACK_MODELS   = [\"gpt-4o\", \"gpt-3.5-turbo-0125\"]\n",
    "MAX_SAMPLES  = 250 # Cap for cost. Use 250 at max\n",
    "REQUEST_DELAY_SEC = 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions\n",
    "\n",
    "def call_chat_model_safely(messages, model):\n",
    "    try:\n",
    "        resp = openai.ChatCompletion.create(model=model, messages=messages, temperature=0)\n",
    "        return resp[\"choices\"][0][\"message\"].get(\"content\",\"\") if resp.get(\"choices\") else \"\"\n",
    "    except Exception as e:\n",
    "        print(f\"[chat:{model}] error: {e}\")\n",
    "        return \"\" \n",
    "\n",
    "def random_seeded_examples(dataset, size=250, seed=42):\n",
    "    random.seed(seed)\n",
    "    return random.sample(dataset, size)\n",
    "\n",
    "def load_json(path: str):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "    \n",
    "def convo_to_transcript(convo: Dict[str,Any]) -> str:\n",
    "    orig = convo.get(\"original\", [])\n",
    "    return \" \".join([f\"{sp}: {tx}\" for sp, tx in orig])\n",
    "\n",
    "def try_parse_json(text: str):\n",
    "    if not text: return None\n",
    "    text = text.strip()\n",
    "    try:\n",
    "        return json.loads(text)\n",
    "    except Exception:\n",
    "        s, e = text.find(\"{\"), text.rfind(\"}\")\n",
    "        if s != -1 and e != -1 and e > s:\n",
    "            cand = text[s:e+1]\n",
    "            try:\n",
    "                return json.loads(cand)\n",
    "            except Exception:\n",
    "                return None\n",
    "    return None\n",
    "\n",
    "def closest_label(pred: str, choices: List[str], cutoff: float = 0.6) -> str:\n",
    "    if not pred or not choices:\n",
    "        return \"\"\n",
    "    if pred in choices:\n",
    "        return pred\n",
    "    # Case-insensitive exact first\n",
    "    lowmap = {c.lower(): c for c in choices}\n",
    "    if pred.lower() in lowmap:\n",
    "        return lowmap[pred.lower()]\n",
    "    # Fuzzy match to nearest valid label\n",
    "    best = difflib.get_close_matches(pred, choices, n=1, cutoff=cutoff)\n",
    "    return best[0] if best else \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset & Split into Train, Dev, and Test\n",
    "\n",
    "abcd = load_json(FILE_PATH)\n",
    "train_split = (abcd.get(\"train\", []) or [])\n",
    "dev_split = (abcd.get(\"dev\", []) or [])\n",
    "test_split = (abcd.get(\"test\", []) or [])\n",
    "print(f\"Train examples: {len(train_split)} | Dev examples: {len(dev_split)} | Test examples: {len(test_split)}\")\n",
    "\n",
    "# Gather 250 random seeded examples of train_split, dev_split, and test_split\n",
    "# Use for consistent sampling during prompt testing, validation, and final evaluation\n",
    "\n",
    "train_seeded_examples = random_seeded_examples(train_split)\n",
    "dev_seeded_examples = random_seeded_examples(dev_split)\n",
    "test_seeded_examples = random_seeded_examples(test_split)\n",
    "print(f\"Seeded Train examples: {len(train_seeded_examples)} | Seeded Dev examples: {len(dev_seeded_examples)} | Seeded Test examples: {len(test_seeded_examples)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build label sets\n",
    "\n",
    "SCHEMA = {\n",
    "  \"personal\": {\"customer_name\":\"\",\"email\":\"\",\"member_level\":\"\",\"phone\":\"\",\"username\":\"\"},\n",
    "  \"order\": {\"street_address\":\"\",\"full_address\":\"\",\"city\":\"\",\"num_products\":\"\",\"order_id\":\"\",\n",
    "            \"packaging\":\"\",\"payment_method\":\"\",\"products\":\"[]\",\"purchase_date\":\"\",\"state\":\"\",\"zip_code\":\"\"},\n",
    "  \"product\": {\"names\":[],\"amounts\":[]},\n",
    "  \"flow\": \"\",\n",
    "  \"subflow\": \"\"\n",
    "}\n",
    "\n",
    "def extract_unique_values_from_conversations(dataset: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "    # Takes a list of dictionaries(the dataset) and returns one dictionary \n",
    "    # Easy access to unique values of each feature\n",
    "\n",
    "    unique_values = {\n",
    "        dictionary: (\n",
    "            {subdictionary: set() for subdictionary in subdict_content}\n",
    "            if isinstance(subdict_content, dict) else set()\n",
    "        )\n",
    "        for dictionary, subdict_content in SCHEMA.items() # Gathered the dictionary getters from SCHEMA\n",
    "    }\n",
    "\n",
    "    # Iterate through all conversations and collect values\n",
    "    for example in dataset:\n",
    "        scenario = example.get(\"scenario\", {})\n",
    "\n",
    "        for dictionary, subdict_content in SCHEMA.items():\n",
    "            # Handle nested dictionaries (personal, order, product)\n",
    "            if isinstance(subdict_content, dict): \n",
    "                subdict = scenario.get(dictionary, {})\n",
    "                for subdictionary in subdict_content.keys():\n",
    "                    value = subdict.get(subdictionary)\n",
    "                    if value not in (None, \"\"):\n",
    "                        # If value is a list, add each item individually\n",
    "                        if isinstance(value, list):\n",
    "                            unique_values[dictionary][subdictionary].update(value)\n",
    "                        else:\n",
    "                            unique_values[dictionary][subdictionary].add(value)\n",
    "            # Handle non-nested dictionary (flow, subflow)\n",
    "            else:\n",
    "                value = scenario.get(dictionary)\n",
    "                if value not in (None, \"\"):\n",
    "                    unique_values[dictionary].add(value)\n",
    "\n",
    "    return unique_values\n",
    "\n",
    "label_sets = extract_unique_values_from_conversations(train_split)\n",
    "\n",
    "def enrich_with_ontology(unique_values: Dict[str, Any], ontology_path: str) -> Dict[str, Any]:\n",
    "    try:\n",
    "        with open(ontology_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            ontology = json.load(f)\n",
    "\n",
    "        def walk_and_add(item, path=[]):\n",
    "            # If it's a dictionary, dive into each key-value pair\n",
    "            if isinstance(item, dict):\n",
    "                for key, value in item.items():\n",
    "                    walk_and_add(value, path + [key])\n",
    "            # If it's a list, check each element\n",
    "            elif isinstance(item, list):\n",
    "                for element in item:\n",
    "                    walk_and_add(element, path)\n",
    "            # If it's a string, add it to matching feature sets\n",
    "            elif isinstance(item, str):\n",
    "                for section, content in unique_values.items():\n",
    "                    if isinstance(content, dict):\n",
    "                        for field_name in content.keys():\n",
    "                            if field_name in path or section in path:\n",
    "                                unique_values[section][field_name].add(item)\n",
    "                    else:\n",
    "                        if section in path:\n",
    "                            unique_values[section].add(item)\n",
    "\n",
    "        walk_and_add(ontology)\n",
    "        print(f\"[ontology] Successfully enriched features with values from {ontology_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[ontology] Could not parse {ontology_path}: {e}\")\n",
    "\n",
    "    return unique_values\n",
    "\n",
    "label_sets = enrich_with_ontology(label_sets, ONTOLOGY_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metadata extractor\n",
    "\n",
    "# Full list of features. Only use the features you are responsible for\n",
    "label_opts = {\n",
    "\"customer_name\": f\"- Valid customer_name labels (pick exactly one, copy verbatim): {label_sets['personal']['customer_name']}\\n\",\n",
    "\"email\": f\"- Valid email labels (pick exactly one, copy verbatim): {label_sets['personal']['email']}\\n\",\n",
    "\"memeber_level\": f\"- Valid member_level labels (pick exactly one, copy verbatim): {label_sets['personal']['member_level']}\\n\",\n",
    "\"phone\": f\"- Valid phone labels (pick exactly one, copy verbatim): {label_sets['personal']['phone']}\\n\",\n",
    "\"username\": f\"- Valid username labels (pick exactly one, copy verbatim): {label_sets['personal']['username']}\\n\",\n",
    "\"street_address\": f\"- Valid street_address labels (pick exactly one, copy verbatim): {label_sets['order']['street_address']}\\n\",\n",
    "\"full_address\": f\"- Valid full_address labels (pick exactly one, copy verbatim): {label_sets['order']['full_address']}\\n\",\n",
    "\"city\": f\"- Valid city labels (pick exactly one, copy verbatim): {label_sets['order']['city']}\\n\",\n",
    "\"num_products\": f\"- Valid num_products labels (pick exactly one, copy verbatim): {label_sets['order']['num_products']}\\n\",\n",
    "\"order_id\": f\"- Valid order_id labels (pick exactly one, copy verbatim): {label_sets['order']['order_id']}\\n\",\n",
    "\"packaging\": f\"- Valid packaging labels (pick exactly one, copy verbatim): {label_sets['order']['packaging']}\\n\",\n",
    "\"payment_method\": f\"- Valid payment_method labels (pick exactly one, copy verbatim): {label_sets['order']['payment_method']}\\n\",\n",
    "\"products\": f\"- Valid products labels (pick exactly one, copy verbatim): {label_sets['order']['products']}\\n\",\n",
    "\"purchase_date\": f\"- Valid purchase_date labels (pick exactly one, copy verbatim): {label_sets['order']['purchase_date']}\\n\",\n",
    "\"state\": f\"- Valid state labels (pick exactly one, copy verbatim): {label_sets['order']['state']}\\n\",\n",
    "\"zip_code\": f\"- Valid zip_code labels (pick exactly one, copy verbatim): {label_sets['order']['zip_code']}\\n\",\n",
    "\"names\": f\"- Valid names labels (pick exactly one, copy verbatim): {label_sets['product']['names']}\\n\",\n",
    "\"amounts\": f\"- Valid amounts labels (pick exactly one, copy verbatim): {label_sets['product']['amounts']}\\n\",\n",
    "\"flow\": f\"- Valid flow labels (pick exactly one, copy verbatim): {label_sets['flow']}\\n\",\n",
    "\"subflow\": f\"- Valid subflow labels (pick exactly one, copy verbatim): {label_sets['subflow']}\\n\"    \n",
    "}\n",
    "fewshot_examples = \"\"\"\n",
    "EXAMPLES:\n",
    "\n",
    "Example 1:\n",
    "Transcript:\n",
    "\\\"\\\"\\\"\n",
    "customer: Hi! I need to return an item, can you help me with that?\n",
    "agent: sure, may I have your name please?\n",
    "customer: I got the wrong size.\n",
    "\\\"\\\"\\\"\n",
    "Correct flow: product_defect\n",
    "Correct subflow: return_size\n",
    "\n",
    "Example 2:\n",
    "Transcript:\n",
    "\\\"\\\"\\\"\n",
    "customer: just wanted to check on the status of a refund\n",
    "agent: everything in order, soon I will indicate the status of your refund.\n",
    "customer: how much long till it is refunded\n",
    "\\\"\\\"\\\"\n",
    "Correct flow: product_defect\n",
    "Correct subflow: refund_status\n",
    "\n",
    "Example 3:\n",
    "Transcript:\n",
    "\\\"\\\"\\\"\n",
    "customer: I've got a promo code and I want to know when they expire.\n",
    "agent: sure! let me check that.\n",
    "agent: Ok, all promo codes expire after 7 days without fail.\n",
    "\\\"\\\"\\\"\n",
    "Correct flow: storewide_query\n",
    "Correct subflow: timing_4\n",
    "\"\"\"\n",
    "\n",
    "def extract_metadata_from_transcript(transcript: str, opts: list, label_sets: dict=label_sets) -> dict:    \n",
    "    selected_instr = ''.join(label_opts[opt] for opt in opts if opt in label_opts)\n",
    "\n",
    "    # Add task-specific guidance for subflow / flow\n",
    "    classification_guidelines = \"\"\n",
    "    if \"subflow\" in opts:\n",
    "        classification_guidelines += (\n",
    "        \"SUBFLOW CLASSIFICATION GUIDELINES:\\n\"\n",
    "        \"- Base the subflow ONLY on the CUSTOMER'S main goal, not on the agent's mistakes or speculation.\\n\"\n",
    "        \"- Ignore greetings, small talk, politeness, apologies, and generic empathy.\\n\"\n",
    "        \"- If multiple issues are mentioned, choose the one the customer repeats or emphasizes most.\\n\"\n",
    "        \"- If the customer is changing or cancelling something BEFORE delivery, prefer subflows related to\\n\"\n",
    "        \"  change-address, modify-order, or cancel-order.\\n\"\n",
    "        \"- If the customer reports a problem AFTER delivery (missing item, damaged item, wrong size, stain,\\n\"\n",
    "        \"  refund questions), choose the corresponding post-delivery subflow.\\n\"\n",
    "        \"- Always choose EXACTLY ONE subflow label. Do NOT output multiple labels.\\n\\n\"\n",
    "\n",
    "        \"SUBFLOW KEYWORD MAPPING RULES:\\n\"\n",
    "        \"- refund_status → Use when the customer asks WHEN a refund will be processed, the timing of a refund,\\n\"\n",
    "        \"  or the progress of a refund.\\n\"\n",
    "        \"- refund_update → Use when the customer already HAS a refund in progress and wants an UPDATE.\\n\"\n",
    "        \"- return_size → Wrong size, need a different size, too small, too large, incorrect fit.\\n\"\n",
    "        \"- timing_4 → Promo code timing questions, expiration dates, offer deadlines.\\n\"\n",
    "        \"- status_due_amount → When the customer asks HOW MUCH they owe or the billing amount.\\n\"\n",
    "        \"- status_due_date → When the customer asks WHEN a bill, subscription, or payment is due.\\n\"\n",
    "        \"- manage_change_address → When the customer wants to update or correct their shipping address.\\n\"\n",
    "        \"- manage_upgrade → When the customer wants to upgrade or modify an order BEFORE shipment.\\n\"\n",
    "        \"- status_credit_missing → When the customer is missing expected credits or account adjustments.\\n\"\n",
    "        \"- status_delivery_time → Questions about WHEN an order will arrive or delivery timing.\\n\"\n",
    "        \"- shopping_cart → Issues adding/removing items, checkout problems, or cart not updating.\\n\\n\"\n",
    "\n",
    "        \"FINE-GRAINED LABEL SELECTION RULE:\\n\"\n",
    "        \"- When labels share the same prefix (e.g., jacket_how_1 vs jacket_how_4), choose the variant that\\n\"\n",
    "        \"  BEST MATCHES the CUSTOMER'S specific question.\\n\"\n",
    "        \"- Do NOT default automatically to the `_1` or `_status` version.\\n\\n\"\n",
    "    )\n",
    "\n",
    "\n",
    "    if \"flow\" in opts:\n",
    "        classification_guidelines += (\n",
    "            \"FLOW CLASSIFICATION GUIDELINES:\\n\"\n",
    "            \"- The flow is the high-level category of the interaction (e.g., order_issue, subscription, etc.).\\n\"\n",
    "            \"- Pick the single most appropriate flow that best groups the customer’s main request.\\n\\n\"\n",
    "        )\n",
    "\n",
    "    label_instr = (\n",
    "        \"CLASSIFICATION CONSTRAINTS:\\n\"\n",
    "        + classification_guidelines\n",
    "        + selected_instr +\n",
    "        \"- Do NOT invent new labels. Use only the valid labels above.\\n\"\n",
    "        \"- If uncertain, pick the single most likely label.\\n\"\n",
    "    )\n",
    "\n",
    "    prompt = (\n",
    "        \"Convert the following customer-support dialog into structured metadata.\\n\\n\"\n",
    "        f\"{fewshot_examples}\\n\"\n",
    "        f\"{label_instr}\\n\"\n",
    "        \"OUTPUT RULES:\\n\"\n",
    "        \"- Return STRICT JSON only (no prose, no markdown).\\n\"\n",
    "        \"- Use this exact schema and field types:\\n\"\n",
    "        f\"{json.dumps(SCHEMA, separators=(',', ':'))}\\n\"\n",
    "        \"- For fields you are not sure about, you may leave them as \\\"\\\" or [].\\n\"\n",
    "        \"- Your TOP PRIORITY is to correctly classify 'subflow' (and 'flow' if requested).\\n\\n\"\n",
    "        f\"Dialog transcript:\\n{transcript}\\n\"\n",
    "    )\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": (\n",
    "                \"You are an information extraction and classification model. \"\n",
    "                \"Always return valid JSON that exactly matches the schema. \"\n",
    "                \"Do not add explanations or comments.\"\n",
    "            ),\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "    \n",
    "\n",
    "    # Try primary and fallback models\n",
    "    models_to_try = [PRIMARY_MODEL] + [m for m in FALLBACK_MODELS if m != PRIMARY_MODEL]\n",
    "\n",
    "    for model_name in models_to_try:\n",
    "        content = call_chat_model_safely(messages, model_name)\n",
    "        data = try_parse_json(content)\n",
    "        if isinstance(data, dict):\n",
    "            # Deep copy schema to ensure fresh structure\n",
    "            out = copy.deepcopy(SCHEMA)\n",
    "            # Populate with model output\n",
    "            for key, value in data.items():\n",
    "                out[key] = value\n",
    "\n",
    "        # Post-process: normalize labels using closest_label()\n",
    "            for key, value in out.items():\n",
    "                if key in label_sets and isinstance(value, str):\n",
    "                    out[key] = closest_label(value, label_sets[key])\n",
    "                elif key in label_sets and isinstance(value, list):\n",
    "                    out[key] = [closest_label(v, label_sets[key]) for v in value]\n",
    "\n",
    "            return out\n",
    "        \n",
    "        if content:\n",
    "            print(f\"[warn:{model_name}] unparsable output (first 160 chars): {content[:160]}\")\n",
    "        time.sleep(REQUEST_DELAY_SEC)\n",
    "\n",
    "    # If no model produced valid JSON, return empty schema\n",
    "    return copy.deepcopy(SCHEMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep excess or experimental prompt/extraction code here (not executed). CLEAR EVERYTHING BELOW THIS LINE BEFORE PUSHING TO MAIN!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential prompt testing setup functions\n",
    "\n",
    "# Dataframe creator\n",
    "def build_dataframe(examples: list, features_to_include: list, max_samples: int=MAX_SAMPLES):\n",
    "    rows = []\n",
    "    features_to_include = set(features_to_include) if features_to_include else None # A set looks up the dictionary keys faster than a list does\n",
    "\n",
    "    for example in examples[:max_samples]:\n",
    "\n",
    "        scenario = example.get(\"scenario\", {})\n",
    "        # Flatten scenario keys\n",
    "        personal = scenario.get(\"personal\", {})\n",
    "        order = scenario.get(\"order\", {})\n",
    "        product = scenario.get(\"product\", {})\n",
    "\n",
    "        features = {\n",
    "            \"customer_name\": personal.get(\"customer_name\"),\n",
    "            \"email\": personal.get(\"email\"),\n",
    "            \"member_level\": personal.get(\"member_level\"),\n",
    "            \"phone\": personal.get(\"phone\"),\n",
    "            \"username\": personal.get(\"username\"),\n",
    "            \"street_address\": order.get(\"street_address\"),\n",
    "            \"full_address\": order.get(\"full_address\"),\n",
    "            \"city\": order.get(\"city\"),\n",
    "            \"num_products\": order.get(\"num_products\"),\n",
    "            \"order_id\": order.get(\"order_id\"),\n",
    "            \"packaging\": order.get(\"packaging\"),\n",
    "            \"payment_method\": order.get(\"payment_method\"),\n",
    "            \"products\": order.get(\"products\"),\n",
    "            \"purchase_date\": order.get(\"purchase_date\"),\n",
    "            \"state\": order.get(\"state\"),\n",
    "            \"zip_code\": order.get(\"zip_code\"),\n",
    "            \"names\": product.get(\"names\"),\n",
    "            \"amounts\": product.get(\"amounts\"),\n",
    "            \"flow\": scenario.get(\"flow\"),\n",
    "            \"subflow\": scenario.get(\"subflow\"),\n",
    "        }\n",
    "\n",
    "        # This code keeps only the features you want in the features dictionary\n",
    "        if features_to_include:\n",
    "            features = {key: value for key, value in features.items() if key in features_to_include}\n",
    "        rows.append({\n",
    "            \"convo_id\": example.get(\"convo_id\",\"\"),\n",
    "            **features,\n",
    "            \"transcript\": convo_to_transcript(example),\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# Make predictions (extraction)\n",
    "def extraction(df, feature_options : list):\n",
    "    predicted_feature_values = []\n",
    "    for transcript in tqdm(df[\"transcript\"], desc=\"Extracting Metadata\"):\n",
    "        predicted_feature_values.append(extract_metadata_from_transcript(transcript, feature_options))\n",
    "        time.sleep(REQUEST_DELAY_SEC)  \n",
    "    \n",
    "    # Makes feature names appear cleanly (example: \"email\" instead of \"personal.email\")\n",
    "    extracted = []\n",
    "    for section in [\"personal\", \"order\", \"product\", \"flow\", \"subflow\"]:\n",
    "        if section in [\"flow\", \"subflow\"]:\n",
    "            extract = pd.DataFrame({f\"extracted_{section}\": [item[section] for item in predicted_feature_values]})\n",
    "        else:\n",
    "            extract = pd.json_normalize([item[section] for item in predicted_feature_values]).add_prefix(\"extracted_\") \n",
    "        extracted.append(extract)\n",
    "    extracted_df = pd.concat(extracted, axis=1).reset_index(drop=True)\n",
    "    \n",
    "    final_df = pd.concat([df.reset_index(drop=True), extracted_df[[f\"extracted_{f}\" for f in feature_options]]], axis=1)\n",
    "\n",
    "    return final_df\n",
    "\n",
    "# Measures extraction accuracy per feature\n",
    "def accuracy(df, feature_options: list):\n",
    "    for feature in feature_options:\n",
    "        gt = df[feature].astype(str).fillna(\"\")\n",
    "        ex = df[f\"extracted_{feature}\"].astype(str).fillna(\"\")\n",
    "        acc = (gt == ex).mean()\n",
    "        print(f\"{feature} accuracy: {acc: .2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 1️. TRAINING EVALUATION\n",
    "# ============================================================\n",
    "# Purpose: Test your extraction function (extract_metadata_from_transcript)\n",
    "#          on training examples to refine prompt behavior and schema.\n",
    "# Notes:\n",
    "# - Run this cell often while tuning prompts.\n",
    "# - Expect to iterate and modify the extraction prompt/function here.\n",
    "# ============================================================\n",
    "\n",
    "feature_options = [\"subflow\", \"flow\"] # Include the features you are supposed to test (ex. [\"flow\", \"subflow\"]). CLEAR VALUE BEFORE PUSHING FILE TO MAIN!!!\n",
    "examples = 50 # Can use variable as 3rd arugment for build_dataframe (default is 250). You do not have to rerun prior code cells\n",
    "\n",
    "df = build_dataframe(train_seeded_examples, feature_options, examples)\n",
    "print(\"DataFrame shape:\", df.shape)\n",
    "\n",
    "final_df = extraction(df, feature_options)\n",
    "pd.set_option(\"display.max_colwidth\", 25)\n",
    "display(final_df.head(250))\n",
    "\n",
    "accuracy(final_df, feature_options)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 2️. DEVELOPMENT EVALUATION\n",
    "# ============================================================\n",
    "# Purpose: Evaluate your *current best* prompt on development examples.\n",
    "# Notes:\n",
    "# - Dev data is unseen during tuning.\n",
    "# - Run this cell only after training accuracy stabilizes.\n",
    "# - Checks whether your prompt generalizes well.\n",
    "# ============================================================\n",
    "\n",
    "df = build_dataframe(dev_seeded_examples, feature_options, examples)\n",
    "print(\"DataFrame shape:\", df.shape)\n",
    "\n",
    "final_df = extraction(df, feature_options)\n",
    "pd.set_option(\"display.max_colwidth\", 25)\n",
    "display(final_df.head(250))\n",
    "\n",
    "accuracy(final_df, feature_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 3️. FINAL TEST EVALUATION\n",
    "# ============================================================\n",
    "# Purpose: Final evaluation on test data after dev results look good.\n",
    "# Notes:\n",
    "# - Use this cell only once prompt tuning is complete.\n",
    "# - Keep it clean and reproducible.\n",
    "# ============================================================\n",
    "\n",
    "df = build_dataframe(test_seeded_examples, feature_options, examples)\n",
    "print(\"DataFrame shape:\", df.shape)\n",
    "\n",
    "final_df = extraction(df, feature_options)\n",
    "pd.set_option(\"display.max_colwidth\", 25)\n",
    "display(final_df.head(250))\n",
    "\n",
    "accuracy(final_df, feature_options)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
