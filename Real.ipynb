{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === TEST-SET EVAL with smart label sets (Colab, OpenAI 0.28, no env vars) ===\n",
    "!pip -q install \"openai==0.28\" pandas tqdm\n",
    "\n",
    "import os, json, gzip, time, difflib\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from typing import Dict, Any, List, Tuple\n",
    "from getpass import getpass\n",
    "import openai\n",
    "\n",
    "print(\"OpenAI SDK version:\", openai.__version__)\n",
    "\n",
    "# -----------------------------\n",
    "# Config\n",
    "# -----------------------------\n",
    "FILE_PATH         = \"abcd_sample (2).json\"   # use full dataset if you have it; fallback to \"abcd_sample.json\"\n",
    "ONTOLOGY_PATH     = \"data/ontology.json\"    # optional; enrich label sets if present\n",
    "PRIMARY_MODEL     = \"gpt-4o\"           # try \"gpt-4o\" if you have access\n",
    "FALLBACK_MODELS   = [\"gpt-4o\", \"gpt-3.5-turbo-0125\"]\n",
    "MAX_TEST_SAMPLES  = 50                      # cap for cost\n",
    "REQUEST_DELAY_SEC = 0.7\n",
    "\n",
    "# -----------------------------\n",
    "# API key (no env vars)\n",
    "# -----------------------------\n",
    "openai.api_key = getpass(\"Enter your OpenAI API key (will not echo): \")\n",
    "\n",
    "def safe_chat(messages, model):\n",
    "    try:\n",
    "        resp = openai.ChatCompletion.create(model=model, messages=messages, temperature=0)\n",
    "        return resp[\"choices\"][0][\"message\"].get(\"content\",\"\") if resp.get(\"choices\") else \"\"\n",
    "    except Exception as e:\n",
    "        print(f\"[chat:{model}] error: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# -----------------------------\n",
    "# IO helpers\n",
    "# -----------------------------\n",
    "def load_json_maybe_gz(path: str):\n",
    "    if not os.path.exists(path):\n",
    "        gz = path + \".gz\"\n",
    "        if os.path.exists(gz):\n",
    "            path = gz\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"Could not find {path}\")\n",
    "    with open(path, \"rb\") as f:\n",
    "        is_gz = f.read(2) == b\"\\x1f\\x8b\"\n",
    "    opener = gzip.open if is_gz else open\n",
    "    with opener(path, \"rt\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def convo_to_transcript(convo: Dict[str,Any]) -> str:\n",
    "    orig = convo.get(\"original\", [])\n",
    "    return \" \".join([f\"{sp}: {tx}\" for sp, tx in orig])\n",
    "\n",
    "# -----------------------------\n",
    "# Load dataset\n",
    "# -----------------------------\n",
    "try:\n",
    "    abcd = load_json_maybe_gz(FILE_PATH)\n",
    "except FileNotFoundError:\n",
    "    # fallback: sample file in CWD\n",
    "    FILE_PATH = \"abcd_sample.json\"\n",
    "    abcd = load_json_maybe_gz(FILE_PATH)\n",
    "\n",
    "sample_mode = isinstance(abcd, list)\n",
    "if sample_mode:\n",
    "    print(\"Detected sample-style file (list). We'll split it 50/50 for demo.\")\n",
    "    n = len(abcd)\n",
    "    train_dev = abcd[: max(1, n//2)]\n",
    "    test_split = abcd[max(1, n//2):]\n",
    "else:\n",
    "    train_dev = (abcd.get(\"train\", []) or []) + (abcd.get(\"dev\", []) or [])\n",
    "    test_split = (abcd.get(\"test\", []) or [])\n",
    "\n",
    "print(f\"Train+Dev convos: {len(train_dev)} | Test convos: {len(test_split)}\")\n",
    "if len(test_split) == 0:\n",
    "    raise RuntimeError(\"No test split found. Point FILE_PATH to abcd_v1.1.json(.gz) or keep sample file.\")\n",
    "\n",
    "# -----------------------------\n",
    "# Build label sets\n",
    "#  - full dataset: from train+dev only (no test leakage)\n",
    "#  - sample file (very tiny): if too few labels found, broaden using ALL items in sample\n",
    "#  - ontology.json (if present): optionally enrich choices\n",
    "# -----------------------------\n",
    "def labels_from_convos(convos: List[Dict[str,Any]]) -> Tuple[List[str], List[str]]:\n",
    "    flows, subs = set(), set()\n",
    "    for c in convos:\n",
    "        sc = c.get(\"scenario\", {})\n",
    "        f  = sc.get(\"flow\", \"\")\n",
    "        sf = sc.get(\"subflow\", \"\")\n",
    "        if f:  flows.add(str(f))\n",
    "        if sf: subs.add(str(sf))\n",
    "    return sorted(flows), sorted(subs)\n",
    "\n",
    "flow_opts, subflow_opts = labels_from_convos(train_dev)\n",
    "\n",
    "# Optional: enrich from ontology.json (won't leak test labels semantically, just adds known valid strings)\n",
    "if os.path.exists(ONTOLOGY_PATH):\n",
    "    try:\n",
    "        onto = load_json_maybe_gz(ONTOLOGY_PATH)\n",
    "        strings = set()\n",
    "        def walk(x):\n",
    "            if isinstance(x, dict):\n",
    "                for k,v in x.items():\n",
    "                    if isinstance(k,str): strings.add(k)\n",
    "                    walk(v)\n",
    "            elif isinstance(x, list):\n",
    "                for i in x: walk(i)\n",
    "            elif isinstance(x, str):\n",
    "                strings.add(x)\n",
    "        walk(onto)\n",
    "        # Keep only ontology strings that look like our labels (heuristic: must contain underscore or be present in any split)\n",
    "        all_fl, all_sf = labels_from_convos(((abcd if sample_mode else (abcd.get(\"train\", []) + abcd.get(\"dev\", []) + abcd.get(\"test\", []))) if abcd else []))\n",
    "        candidates = {s for s in strings if (\"_\" in s) or (s in all_fl) or (s in all_sf)}\n",
    "        flow_opts = sorted(set(flow_opts) | ( candidates & set(all_fl) ))\n",
    "        subflow_opts = sorted(set(subflow_opts) | ( candidates & set(all_sf) ))\n",
    "    except Exception as e:\n",
    "        print(f\"[ontology] Could not parse {ONTOLOGY_PATH}: {e}. Continuing with train/dev labels.\")\n",
    "\n",
    "# If sample is too tiny (e.g., only 1 label each), broaden using all items in sample\n",
    "if sample_mode and (len(flow_opts) < 2 or len(subflow_opts) < 2):\n",
    "    all_fl, all_sf = labels_from_convos(abcd)\n",
    "    if len(flow_opts) < 2:     flow_opts = all_fl\n",
    "    if len(subflow_opts) < 2:  subflow_opts = all_sf\n",
    "    print(\"[sample] Broadened label sets using all sample convos.\")\n",
    "\n",
    "print(f\"Flow label count: {len(flow_opts)} | Subflow label count: {len(subflow_opts)}\")\n",
    "\n",
    "# -----------------------------\n",
    "# JSON schema + parsing\n",
    "# -----------------------------\n",
    "SCHEMA = {\n",
    "  \"personal\": {\"customer_name\":\"\",\"email\":\"\",\"member_level\":\"\",\"phone\":\"\",\"username\":\"\"},\n",
    "  \"order\": {\"street_address\":\"\",\"full_address\":\"\",\"city\":\"\",\"num_products\":\"\",\"order_id\":\"\",\n",
    "            \"packaging\":\"\",\"payment_method\":\"\",\"products\":\"[]\",\"purchase_date\":\"\",\"state\":\"\",\"zip_code\":\"\"},\n",
    "  \"product\": {\"names\":[],\"amounts\":[]},\n",
    "  \"flow\": \"\",\n",
    "  \"subflow\": \"\"\n",
    "}\n",
    "\n",
    "def try_parse_json(text: str):\n",
    "    if not text: return None\n",
    "    text = text.strip()\n",
    "    try:\n",
    "        return json.loads(text)\n",
    "    except Exception:\n",
    "        s, e = text.find(\"{\"), text.rfind(\"}\")\n",
    "        if s != -1 and e != -1 and e > s:\n",
    "            cand = text[s:e+1]\n",
    "            try:\n",
    "                return json.loads(cand)\n",
    "            except Exception:\n",
    "                return None\n",
    "    return None\n",
    "\n",
    "def closest_label(pred: str, choices: List[str], cutoff: float = 0.6) -> str:\n",
    "    if not pred or not choices:\n",
    "        return \"\"\n",
    "    if pred in choices:\n",
    "        return pred\n",
    "    # case-insensitive exact first\n",
    "    lowmap = {c.lower(): c for c in choices}\n",
    "    if pred.lower() in lowmap:\n",
    "        return lowmap[pred.lower()]\n",
    "    # fuzzy match to nearest valid label\n",
    "    best = difflib.get_close_matches(pred, choices, n=1, cutoff=cutoff)\n",
    "    return best[0] if best else \"\"\n",
    "\n",
    "# -----------------------------\n",
    "# Extractor with constrained choices\n",
    "# -----------------------------\n",
    "def extract_metadata_from_transcript(transcript: str,\n",
    "                                     flow_choices: List[str],\n",
    "                                     subflow_choices: List[str]) -> Dict[str, Any]:\n",
    "    label_instr = (\n",
    "        \"CLASSIFICATION CONSTRAINTS:\\n\"\n",
    "        f\"- Valid flow labels (pick exactly one, copy verbatim): {flow_choices}\\n\"\n",
    "        f\"- Valid subflow labels (pick exactly one, copy verbatim): {subflow_choices}\\n\"\n",
    "        \"- Do NOT invent new labels. If uncertain, pick the most likely from the lists.\\n\"\n",
    "    )\n",
    "    prompt = (\n",
    "        \"Convert the customer-support dialog into structured metadata.\\n\\n\"\n",
    "        f\"{label_instr}\\n\"\n",
    "        \"OUTPUT RULES:\\n\"\n",
    "        \"- Return STRICT JSON only (no prose, no markdown).\\n\"\n",
    "        \"- Use this exact schema and field types:\\n\"\n",
    "        f\"{json.dumps(SCHEMA, indent=2)}\\n\"\n",
    "        \"- If a field is missing, use \\\"\\\" or [] accordingly.\\n\"\n",
    "        \"- 'flow' and 'subflow' MUST be exactly one of the provided labels above.\\n\\n\"\n",
    "        \"Dialog transcript:\\n\"\n",
    "        f\"{transcript}\\n\"\n",
    "    )\n",
    "    messages = [\n",
    "        {\"role\":\"system\",\"content\":\"Always return valid JSON that exactly matches the schema. No explanations.\"},\n",
    "        {\"role\":\"user\",\"content\":prompt}\n",
    "    ]\n",
    "    models_to_try = [PRIMARY_MODEL] + [m for m in FALLBACK_MODELS if m != PRIMARY_MODEL]\n",
    "    for m in models_to_try:\n",
    "        content = safe_chat(messages, m)\n",
    "        data = try_parse_json(content)\n",
    "        if isinstance(data, dict):\n",
    "            out = json.loads(json.dumps(SCHEMA))\n",
    "            for k,v in data.items():\n",
    "                out[k] = v\n",
    "            # enforce / normalize labels\n",
    "            out[\"flow\"]    = closest_label(out.get(\"flow\",\"\"), flow_choices, cutoff=0.6)\n",
    "            out[\"subflow\"] = closest_label(out.get(\"subflow\",\"\"), subflow_choices, cutoff=0.6)\n",
    "            return out\n",
    "        if content:\n",
    "            print(f\"[warn:{m}] unparsable output (first 160 chars): {content[:160]}\")\n",
    "        time.sleep(REQUEST_DELAY_SEC)\n",
    "    return json.loads(json.dumps(SCHEMA))\n",
    "\n",
    "# -----------------------------\n",
    "# Build TEST dataframe\n",
    "# -----------------------------\n",
    "test_rows = []\n",
    "for convo in test_split[:MAX_TEST_SAMPLES]:\n",
    "    sc = convo.get(\"scenario\", {})\n",
    "    test_rows.append({\n",
    "        \"convo_id\": convo.get(\"convo_id\",\"\"),\n",
    "        \"flow\": sc.get(\"flow\",\"\"),\n",
    "        \"subflow\": sc.get(\"subflow\",\"\"),\n",
    "        \"transcript\": convo_to_transcript(convo)\n",
    "    })\n",
    "test_df = pd.DataFrame(test_rows)\n",
    "print(\"Test DataFrame shape:\", test_df.shape)\n",
    "\n",
    "# -----------------------------\n",
    "# Predict on TEST only\n",
    "# -----------------------------\n",
    "preds = []\n",
    "for t in tqdm(test_df[\"transcript\"], desc=\"Predicting (test)\"):\n",
    "    preds.append(extract_metadata_from_transcript(t, flow_opts, subflow_opts))\n",
    "    time.sleep(REQUEST_DELAY_SEC)\n",
    "\n",
    "extracted = pd.json_normalize(preds, sep=\"_\").add_prefix(\"extracted_\")\n",
    "final_df  = pd.concat([test_df.reset_index(drop=True), extracted.reset_index(drop=True)], axis=1)\n",
    "\n",
    "display(final_df.head())\n",
    "\n",
    "# -----------------------------\n",
    "# Accuracy\n",
    "# -----------------------------\n",
    "for field in [\"flow\",\"subflow\"]:\n",
    "    gt = final_df[field].astype(str).fillna(\"\")\n",
    "    ex = final_df[f\"extracted_{field}\"].astype(str).fillna(\"\")\n",
    "    acc = (gt == ex).mean()\n",
    "    print(f\"Test {field} accuracy: {acc:.2%}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
